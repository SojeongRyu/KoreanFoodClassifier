comment: data_clean_samenum
model and loss plot -> ./model/model_data_clean_samenum
Namespace(batch_size=110, beta1=0.5, beta2=0.999, comment='data_clean_samenum', dataroot_dir='./data_clean_samenum', epoch=30, gpu_mode=True, lr=0.0002, num_workers=4, result_dir='./results/', sample_num=64, save_dir='./model/', type='train')
[*] Training started
[E000]	loss: 2.336979
[E001]	loss: 2.115677
[E002]	loss: 1.973991
[E003]	loss: 2.595090
[E004]	loss: 1.437700
[E005]	loss: 1.885312
[E006]	loss: 1.384354
[E007]	loss: 1.166350
[E008]	loss: 1.174258
[E009]	loss: 1.033821
[E010]	loss: 1.106823
[E011]	loss: 1.125820
[E012]	loss: 0.941511
[E013]	loss: 0.844903
[E014]	loss: 0.897869
[E015]	loss: 0.707022
[E016]	loss: 0.696297
[E017]	loss: 0.671599
[E018]	loss: 0.696563
[E019]	loss: 0.682518
[E020]	loss: 0.655294
[E021]	loss: 0.658383
[E022]	loss: 0.568389
[E023]	loss: 0.424147
[E024]	loss: 0.546612
[E025]	loss: 0.355799
[E026]	loss: 0.418312
[E027]	loss: 0.303269
[E028]	loss: 0.343416
[E029]	loss: 0.650891
Training finished!... save training results
Total time: 487.97511529922485
Per epoch time: [16.038156986236572, 15.055084466934204, 15.092780351638794, 15.65028429031372, 14.716813087463379, 14.978700399398804, 15.073436498641968, 14.308109045028687, 14.101772785186768, 14.333405017852783, 14.823948860168457, 14.826429605484009, 15.072941064834595, 15.041197061538696, 14.517916917800903, 14.147404909133911, 15.25894021987915, 14.892892599105835, 14.916204452514648, 15.5039644241333, 15.96623682975769, 15.20041275024414, 14.357213020324707, 13.781853199005127, 14.471789121627808, 14.158812522888184, 14.319516658782959, 14.656797409057617, 15.808012008666992, 14.460380792617798]
[*] Training finished
