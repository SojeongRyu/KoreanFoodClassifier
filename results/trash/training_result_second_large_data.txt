comment: second_large_data
model and loss plot -> ./model/model_second_large_data
Namespace(batch_size=100, beta1=0.5, beta2=0.999, comment='second_large_data', dataroot_dir='./data_large/', epoch=30, gpu_mode=True, lr=0.0002, num_workers=4, result_dir='./results/', sample_num=64, save_dir='./model/', type='train')
[*] Training started
[E000]	loss: 2.336730
[E001]	loss: 1.976750
[E002]	loss: 2.026688
[E003]	loss: 1.952602
[E004]	loss: 1.542800
[E005]	loss: 1.366487
[E006]	loss: 1.433974
[E007]	loss: 0.920271
[E008]	loss: 1.074185
[E009]	loss: 1.106894
[E010]	loss: 0.852887
[E011]	loss: 0.861532
[E012]	loss: 0.799383
[E013]	loss: 0.780953
[E014]	loss: 0.718655
[E015]	loss: 0.771791
[E016]	loss: 0.604393
[E017]	loss: 0.676767
[E018]	loss: 0.514720
[E019]	loss: 0.460764
[E020]	loss: 0.349569
[E021]	loss: 0.612346
[E022]	loss: 0.334416
[E023]	loss: 0.316481
[E024]	loss: 0.484501
[E025]	loss: 0.310069
[E026]	loss: 0.324370
[E027]	loss: 0.313395
[E028]	loss: 0.480399
[E029]	loss: 0.326410
Training finished!... save training results
Total time: 1021.1663136482239
Per epoch time: [46.52958631515503, 33.61111927032471, 32.11509966850281, 34.859781980514526, 32.64669895172119, 32.787323236465454, 31.18760085105896, 31.110806703567505, 31.81592106819153, 33.9552001953125, 32.47914695739746, 32.149030447006226, 31.801958560943604, 32.592843532562256, 32.6427104473114, 31.12975549697876, 31.733142375946045, 31.232481241226196, 32.23081135749817, 33.08752083778381, 31.046977281570435, 32.29563856124878, 31.15369200706482, 32.279680490493774, 32.056278705596924, 32.0403208732605, 33.59516215324402, 33.50141406059265, 33.13838481903076, 31.761067628860474]
[*] Training finished
