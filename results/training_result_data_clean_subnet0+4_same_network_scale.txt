(SSS) C:\Users\405B\KFC>python main.py --comment data_clean_subnet0+4_same_network_scale --dataroot_dir ./data_clean --epoch 70 --sub_net_name 0+4 --type train
comment: data_clean_subnet0+4_same_network_scale
model and loss plot -> ./model/model_data_clean_subnet0+4_same_network_scale
Namespace(batch_size=100, beta1=0.9, beta2=0.999, comment='data_clean_subnet0+4_same_network_scale', dataroot_dir='./data_clean', epoch=70, fold_num=-1, gpu_mode=True, lr=0.0002, num_workers=4, result_dir='./results/', sample_num=64, save_dir='./model/', sub_net_name='0+4', type='train')
[*] Training started
[E000]  loss: 2.338808
[E001]  loss: 1.357244
[E002]  loss: 1.443876
[E003]  loss: 1.443956
[E004]  loss: 1.202734
[E005]  loss: 1.116301
[E006]  loss: 1.000521
[E007]  loss: 0.868643
[E008]  loss: 0.777989
[E009]  loss: 0.802586
[E010]  loss: 0.758243
[E011]  loss: 0.613216
[E012]  loss: 0.556118
[E013]  loss: 0.289480
[E014]  loss: 0.404344
[E015]  loss: 0.389221
[E016]  loss: 0.563271
[E017]  loss: 0.365301
[E018]  loss: 0.333667
[E019]  loss: 0.465644
[E020]  loss: 0.241357
[E021]  loss: 0.383419
[E022]  loss: 0.408255
[E023]  loss: 0.269459
[E024]  loss: 0.221170
[E025]  loss: 0.221161
[E026]  loss: 0.153070
[E027]  loss: 0.197735
[E028]  loss: 0.255733
[E029]  loss: 0.254844
[E030]  loss: 0.316830
[E031]  loss: 0.322511
[E032]  loss: 0.207692
[E033]  loss: 0.207992
[E034]  loss: 0.073960
[E035]  loss: 0.108133
[E036]  loss: 0.187231
[E037]  loss: 0.177928
[E038]  loss: 0.129638
[E039]  loss: 0.090603
[E040]  loss: 0.130620
[E041]  loss: 0.330127
[E042]  loss: 0.160135
[E043]  loss: 0.164539
[E044]  loss: 0.082791
[E045]  loss: 0.084053
[E046]  loss: 0.086236
[E047]  loss: 0.173435
[E048]  loss: 0.129931
[E049]  loss: 0.192483
[E050]  loss: 0.120679
[E051]  loss: 0.101662
[E052]  loss: 0.170303
[E053]  loss: 0.052134
[E054]  loss: 0.139271
[E055]  loss: 0.156604
[E056]  loss: 0.142838
[E057]  loss: 0.176299
[E058]  loss: 0.072114
[E059]  loss: 0.133638
[E060]  loss: 0.083536
[E061]  loss: 0.114870
[E062]  loss: 0.058882
[E063]  loss: 0.023604
[E064]  loss: 0.050947
[E065]  loss: 0.089823
[E066]  loss: 0.048807
[E067]  loss: 0.090248
[E068]  loss: 0.024389
[E069]  loss: 0.077019
Training finished!... save training results
Total time: 1461.302372455597
Per epoch time: [20.198357343673706, 19.83906579017639, 18.745577812194824, 19.77658200263977, 19.386049032211304, 19.073622703552246, 18.729958534240723, 19.370426654815674, 20.026521682739258, 24.822266340255737, 20.2764630317688, 21.2918484210968, 20.23150873184204, 19.57350492477417, 19.370431423187256, 20.432676792144775, 19.292319536209106, 20.385812759399414, 20.292084217071533, 20.10462737083435, 19.760960578918457, 18.88616704940796, 19.451632261276245, 19.39550495147705, 19.620515823364258, 19.19859218597412, 18.964273929595947, 18.667466640472412, 18.714332580566406, 20.01090145111084, 19.151729583740234, 18.85492467880249, 19.01113796234131, 19.151729345321655, 19.3704252243042, 19.386048078536987, 19.979657649993896, 18.863333463668823, 19.20779585838318, 19.1985981464386, 20.276463508605957, 19.245456218719482, 19.839065551757812, 19.386047840118408, 19.72971820831299, 19.229835510253906, 20.042144775390625, 19.635988473892212, 19.885929584503174, 20.026522397994995, 20.135871171951294, 19.0892436504364, 19.386048793792725, 19.15172529220581, 19.746068239212036, 19.635989904403687, 19.635990619659424, 19.526639461517334, 19.932793140411377, 19.0736243724823, 19.307941913604736, 19.136109352111816, 19.604746341705322, 19.104864835739136, 19.48482656478882, 19.401670694351196, 19.05799961090088, 20.026522636413574, 19.167349815368652, 19.011136770248413]

[E000]  loss: 0.716777
[E001]  loss: 0.603269
[E002]  loss: 0.555012
[E003]  loss: 0.444701
[E004]  loss: 0.392436
[E005]  loss: 0.449297
[E006]  loss: 0.570952
[E007]  loss: 0.374502
[E008]  loss: 0.381225
[E009]  loss: 0.452254
[E010]  loss: 0.328095
[E011]  loss: 0.319003
[E012]  loss: 0.427340
[E013]  loss: 0.250689
[E014]  loss: 0.258158
[E015]  loss: 0.407833
[E016]  loss: 0.273887
[E017]  loss: 0.271782
[E018]  loss: 0.235073
[E019]  loss: 0.170735
[E020]  loss: 0.258153
[E021]  loss: 0.171184
[E022]  loss: 0.177884
[E023]  loss: 0.174003
[E024]  loss: 0.160644
[E025]  loss: 0.193274
[E026]  loss: 0.220294
[E027]  loss: 0.193308
[E028]  loss: 0.380274
[E029]  loss: 0.361905
[E030]  loss: 0.113071
[E031]  loss: 0.253201
[E032]  loss: 0.066589
[E033]  loss: 0.228589
[E034]  loss: 0.151693
[E035]  loss: 0.199877
[E036]  loss: 0.105002
[E037]  loss: 0.140540
[E038]  loss: 0.184695
[E039]  loss: 0.133969
[E040]  loss: 0.086291
[E041]  loss: 0.080899
[E042]  loss: 0.082322
[E043]  loss: 0.106453
[E044]  loss: 0.055914
[E045]  loss: 0.087671
[E046]  loss: 0.068000
[E047]  loss: 0.048635
[E048]  loss: 0.214609
[E049]  loss: 0.035637
[E050]  loss: 0.102712
[E051]  loss: 0.066358
[E052]  loss: 0.051227
[E053]  loss: 0.063608
[E054]  loss: 0.024929
[E055]  loss: 0.106552
[E056]  loss: 0.080994
[E057]  loss: 0.025582
[E058]  loss: 0.139286
[E059]  loss: 0.055278
[E060]  loss: 0.042976
[E061]  loss: 0.003663
[E062]  loss: 0.062993
[E063]  loss: 0.083265
[E064]  loss: 0.043383
[E065]  loss: 0.051306
[E066]  loss: 0.033190
[E067]  loss: 0.015458
[E068]  loss: 0.032479
[E069]  loss: 0.066810
Training finished!... save training results
Total time: 573.770783662796
Per epoch time: [19.104862928390503, 6.826513767242432, 7.201430320739746, 7.07645320892334, 6.8265111446380615, 6.857755422592163, 6.842135429382324, 6.5453290939331055, 6.654684782028198, 6.514085531234741, 6.748408079147339, 6.576573133468628, 7.092075824737549, 6.8108971118927, 6.8265135288238525, 6.607816219329834, 7.404501676559448, 6.732785940170288, 6.764025926589966, 6.764028072357178, 6.623435735702515, 6.514087677001953, 6.7952721118927, 6.717163801193237, 6.545329809188843, 6.482843399047852, 6.795270204544067, 6.764034032821655, 6.732786178588867, 6.639057159423828, 6.998346328735352, 6.81089186668396, 7.076457738876343, 6.779648542404175, 6.920241832733154, 6.670300722122192, 6.764028310775757, 7.0764546394348145, 6.9046196937561035, 6.873375415802002, 6.592194557189941, 6.764028549194336, 6.592193603515625, 6.732784748077393, 6.607814311981201, 6.62343692779541, 6.654680252075195, 6.670299053192139, 6.54533052444458, 6.670307397842407, 6.82651424407959, 6.670300245285034, 6.639056921005249, 6.98272442817688, 6.7327868938446045, 6.748408079147339, 6.545328140258789, 6.639057397842407, 6.514095067977905, 6.8108906745910645, 7.029590606689453, 6.810892105102539, 6.560950756072998, 6.654679536819458, 6.545329809188843, 6.826512575149536, 6.764030694961548, 6.904617786407471, 6.8889970779418945, 6.654677867889404]

[*] Training finished