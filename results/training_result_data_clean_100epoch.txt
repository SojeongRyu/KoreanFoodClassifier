comment: data_clean_100epoch
model and loss plot -> ./model/model_data_clean_100epoch
Namespace(batch_size=200, beta1=0.5, beta2=0.999, comment='data_clean_100epoch', dataroot_dir='./data_clean', epoch=100, fold_num=-1, gpu_mode=True, lr=0.0002, num_workers=4, result_dir='./results/', sample_num=64, save_dir='./model/', type='train')
[*] Training started
[E000]  loss: 2.359689
[E001]  loss: 2.300266
[E002]  loss: 1.911237
[E003]  loss: 1.846566
[E004]  loss: 1.592852
[E005]  loss: 1.511460
[E006]  loss: 1.729021
[E007]  loss: 1.285951
[E008]  loss: 1.217838
[E009]  loss: 1.037060
[E010]  loss: 1.185807
[E011]  loss: 1.056236
[E012]  loss: 0.958751
[E013]  loss: 0.850008
[E014]  loss: 0.809946
[E015]  loss: 0.847806
[E016]  loss: 0.794993
[E017]  loss: 0.789928
[E018]  loss: 0.681071
[E019]  loss: 0.483989
[E020]  loss: 0.674219
[E021]  loss: 0.760640
[E022]  loss: 0.621371
[E023]  loss: 0.766507
[E024]  loss: 0.494502
[E025]  loss: 0.520649
[E026]  loss: 0.523150
[E027]  loss: 0.424662
[E028]  loss: 0.479914
[E029]  loss: 0.469245
[E030]  loss: 0.448113
[E031]  loss: 0.370367
[E032]  loss: 0.400590
[E033]  loss: 0.395544
[E034]  loss: 0.340437
[E035]  loss: 0.214563
[E036]  loss: 0.486229
[E037]  loss: 0.292211
[E038]  loss: 0.345297
[E039]  loss: 0.258937
[E040]  loss: 0.334442
[E041]  loss: 0.366018
[E042]  loss: 0.234109
[E043]  loss: 0.138293
[E044]  loss: 0.263807
[E045]  loss: 0.148697
[E046]  loss: 0.193607
[E047]  loss: 0.098062
[E048]  loss: 0.165291
[E049]  loss: 0.092254
[E050]  loss: 0.181082
[E051]  loss: 0.165836
[E052]  loss: 0.121564
[E053]  loss: 0.247781
[E054]  loss: 0.110622
[E055]  loss: 0.074698
[E056]  loss: 0.195135
[E057]  loss: 0.148026
[E058]  loss: 0.089209
[E059]  loss: 0.069328
[E060]  loss: 0.852498
[E061]  loss: 0.087104
[E062]  loss: 0.089441
[E063]  loss: 0.067752
[E064]  loss: 0.130808
[E065]  loss: 0.120615
[E066]  loss: 0.089224
[E067]  loss: 0.046293
[E068]  loss: 0.061081
[E069]  loss: 0.092570
[E070]  loss: 0.050187
[E071]  loss: 0.021399
[E072]  loss: 2.265249
[E073]  loss: 2.297746
[E074]  loss: 1.906005
[E075]  loss: 1.707896
[E076]  loss: 1.522717
[E077]  loss: 1.136627
[E078]  loss: 1.015475
[E079]  loss: 0.732833
[E080]  loss: 0.737128
[E081]  loss: 0.409065
[E082]  loss: 0.402838
[E083]  loss: 0.248563
[E084]  loss: 0.292664
[E085]  loss: 0.171975
[E086]  loss: 0.176941
[E087]  loss: 0.140564
[E088]  loss: 0.233452
[E089]  loss: 0.140512
[E090]  loss: 0.178377
[E091]  loss: 0.118815
[E092]  loss: 0.133990
[E093]  loss: 0.082034
[E094]  loss: 0.070596
[E095]  loss: 0.127921
[E096]  loss: 0.168898
[E097]  loss: 0.054890
[E098]  loss: 0.043678
[E099]  loss: 0.035797
Training finished!... save training results
Total time: 2162.9672634601593
Per epoch time: [66.3290741443634, 20.40444278717041, 20.152971744537354, 19.786428213119507, 20.664843797683716, 21.89889144897461, 19.69615626335144, 19.709547758102417, 20.100892066955566, 20.860763788223267, 20.75362753868103, 19.390124082565308, 19.71153211593628, 19.572651863098145, 19.680780172348022, 19.677308082580566, 20.594411849975586, 19.324156522750854, 20.26457190513611, 19.655980110168457, 21.00361180305481, 20.20009160041809, 19.35838007926941, 20.24423599243164, 19.145100355148315, 19.745756149291992, 18.95165991783142, 20.06914758682251, 19.524044036865234, 19.45162844657898, 19.65101981163025, 20.472891807556152, 19.74476456642151, 19.9069561958313, 19.7913875579834, 19.381195306777954, 20.488763570785522, 19.889596462249756, 19.373260021209717, 19.67929172515869, 19.117323875427246, 19.534956455230713, 20.253164052963257, 19.953083992004395, 19.761627912521362, 20.539852380752563, 19.117820978164673, 20.355835914611816, 20.89002752304077, 19.822635889053345, 19.64705181121826, 19.171884298324585, 20.169836282730103, 18.78946805000305, 19.453116178512573, 19.76113200187683, 19.04887628555298, 20.48777174949646, 19.58108425140381, 19.65101981163025, 20.311195611953735, 20.02748394012451, 19.6361403465271, 19.58554768562317, 19.692684173583984, 20.40097212791443, 19.555291891098022, 19.170892000198364, 19.992268085479736, 19.502220392227173, 19.16791558265686, 19.833548307418823, 19.426331996917725, 20.01508378982544, 19.621755599975586, 19.75121235847473, 20.922765016555786, 19.31076431274414, 19.80478048324585, 20.847371816635132, 19.137163639068604, 19.915884256362915, 19.822636365890503, 20.203067779541016, 19.788411855697632, 20.27201223373413, 19.387644290924072, 19.7398042678833, 20.01558017730713, 19.352926015853882, 19.55926012992859, 19.545867681503296, 19.14658832550049, 20.338971853256226, 18.996796369552612, 19.6187801361084, 19.36383605003357, 19.67978811264038, 19.769564390182495, 20.8959801197052]

[*] Training finished