comment: hierarchical_cnn_practice
model and loss plot -> ./model/model_hierarchical_cnn_practice
Namespace(batch_size=100, beta1=0.5, beta2=0.999, comment='hierarchical_cnn_practice', dataroot_dir='./data_hierarchy_practice/', epoch=30, gpu_mode=True, lr=0.0002, num_workers=4, result_dir='./results/', sample_num=64, save_dir='./model/', type='train')
[*] Training started
[E000]	loss: 1.777876
[E001]	loss: 1.303940
[E002]	loss: 1.197228
[E003]	loss: 1.244226
[E004]	loss: 1.203064
[E005]	loss: 0.935640
[E006]	loss: 0.950705
[E007]	loss: 1.054076
[E008]	loss: 0.997500
[E009]	loss: 0.784290
[E010]	loss: 0.661203
[E011]	loss: 0.769938
[E012]	loss: 0.602269
[E013]	loss: 0.366760
[E014]	loss: 0.464695
[E015]	loss: 0.461385
[E016]	loss: 0.557387
[E017]	loss: 0.406183
[E018]	loss: 0.632195
[E019]	loss: 0.500147
[E020]	loss: 0.267952
[E021]	loss: 0.272536
[E022]	loss: 0.271259
[E023]	loss: 0.310458
[E024]	loss: 0.310375
[E025]	loss: 0.378242
[E026]	loss: 0.245319
[E027]	loss: 0.283962
[E028]	loss: 0.338502
[E029]	loss: 0.329951
Training finished!... save training results
Total time: 912.6627666950226
Per epoch time: [77.99816179275513, 26.899930000305176, 27.68099594116211, 28.227742433547974, 26.337560653686523, 26.4469096660614, 26.58750295639038, 28.93169069290161, 27.19673228263855, 27.72785758972168, 27.493539094924927, 28.689461946487427, 28.462062120437622, 26.91555380821228, 26.96241331100464, 26.681229829788208, 27.918962001800537, 27.668912410736084, 26.50939679145813, 26.540639877319336, 29.446204662322998, 26.899928331375122, 26.962416887283325, 26.88430666923523, 27.368570566177368, 26.66560935974121, 28.633897066116333, 28.712002754211426, 28.337235927581787, 28.680758476257324]
[E000]	loss: 1.089182
[E001]	loss: 0.981034
[E002]	loss: 1.027335
[E003]	loss: 1.074496
[E004]	loss: 0.948565
[E005]	loss: 0.857820
[E006]	loss: 0.821623
[E007]	loss: 1.330679
[E008]	loss: 0.757843
[E009]	loss: 0.856493
[E010]	loss: 0.682630
[E011]	loss: 0.698955
[E012]	loss: 0.769080
[E013]	loss: 0.626664
[E014]	loss: 0.760726
[E015]	loss: 0.521202
[E016]	loss: 0.563375
[E017]	loss: 0.752802
[E018]	loss: 0.556600
[E019]	loss: 0.424475
[E020]	loss: 0.465232
[E021]	loss: 0.611848
[E022]	loss: 0.735078
[E023]	loss: 0.630260
[E024]	loss: 0.521845
[E025]	loss: 0.468012
[E026]	loss: 0.401576
[E027]	loss: 0.433899
[E028]	loss: 0.403449
[E029]	loss: 0.456564
Training finished!... save training results
Total time: 329.8352885246277
Per epoch time: [9.607117652893066, 9.357172012329102, 9.232207298278809, 9.685224771499634, 9.404041051864624, 9.325937271118164, 9.466525316238403, 9.450904846191406, 10.322506189346313, 9.786345481872559, 10.128265619277954, 9.695174217224121, 9.391953229904175, 10.198511362075806, 10.466289758682251, 9.712225198745728, 10.355631828308105, 10.1548011302948, 9.718128442764282, 9.316105604171753, 9.421954870223999, 9.603970050811768, 9.406537294387817, 9.702738046646118, 9.264374256134033, 9.779049396514893, 9.925165176391602, 9.636949300765991, 9.482475757598877, 9.89718747138977]
[E000]	loss: 1.136903
[E001]	loss: 1.079181
[E002]	loss: 1.070395
[E003]	loss: 1.053101
[E004]	loss: 1.047937
[E005]	loss: 0.781800
[E006]	loss: 0.748774
[E007]	loss: 0.736494
[E008]	loss: 1.023204
[E009]	loss: 0.841112
[E010]	loss: 0.767330
[E011]	loss: 0.708157
[E012]	loss: 0.690105
[E013]	loss: 0.739337
[E014]	loss: 0.722636
[E015]	loss: 0.648258
[E016]	loss: 0.944199
[E017]	loss: 0.758690
[E018]	loss: 0.586066
[E019]	loss: 0.553715
[E020]	loss: 0.550196
[E021]	loss: 0.494002
[E022]	loss: 0.804896
[E023]	loss: 0.340690
[E024]	loss: 0.424470
[E025]	loss: 0.425287
[E026]	loss: 0.478468
[E027]	loss: 0.402294
[E028]	loss: 0.430543
[E029]	loss: 0.275286
Training finished!... save training results
Total time: 317.0476052761078
Per epoch time: [10.40398097038269, 9.603354692459106, 10.113187074661255, 9.203545331954956, 9.288424015045166, 9.783653259277344, 9.123594045639038, 8.969265460968018, 9.564033508300781, 8.999542951583862, 9.133140325546265, 9.078486442565918, 9.032471179962158, 9.227044105529785, 9.07616400718689, 9.647231817245483, 9.019983291625977, 9.37890338897705, 9.228570938110352, 9.423799753189087, 9.780981063842773, 8.875087022781372, 8.921758890151978, 8.984102010726929, 8.937234878540039, 9.265610456466675, 9.249770402908325, 9.172088146209717, 8.921989440917969, 8.874543905258179]
[*] Training finished
