comment: data_clean_70epoch_0.9beta1_subnet0+4
model and loss plot -> ./model/model_data_clean_70epoch_0.9beta1_subnet0+4
Namespace(batch_size=100, beta1=0.9, beta2=0.999, comment='data_clean_70epoch_0.9beta1_subnet0+4', dataroot_dir='./data_clean', epoch=70, fold_num=-1, gpu_mode=True, lr=0.0002, num_workers=4, result_dir='./results/', sample_num=64, save_dir='./model/', sub_net_name='0+4', type='train')
[*] Training started
[E000]  loss: 2.323631
[E001]  loss: 1.486999
[E002]  loss: 1.407400
[E003]  loss: 1.008938
[E004]  loss: 1.152303
[E005]  loss: 0.997162
[E006]  loss: 0.819160
[E007]  loss: 0.948885
[E008]  loss: 0.650211
[E009]  loss: 0.704327
[E010]  loss: 0.631629
[E011]  loss: 0.663615
[E012]  loss: 0.720733
[E013]  loss: 0.615905
[E014]  loss: 0.507587
[E015]  loss: 0.482434
[E016]  loss: 0.399376
[E017]  loss: 0.433155
[E018]  loss: 0.549245
[E019]  loss: 0.394416
[E020]  loss: 0.408155
[E021]  loss: 0.438603
[E022]  loss: 0.236278
[E023]  loss: 0.369031
[E024]  loss: 0.206537
[E025]  loss: 0.381140
[E026]  loss: 0.307078
[E027]  loss: 0.411523
[E028]  loss: 0.136259
[E029]  loss: 0.347020
[E030]  loss: 0.263040
[E031]  loss: 0.169276
[E032]  loss: 0.267473
[E033]  loss: 0.195807
[E034]  loss: 0.141496
[E035]  loss: 0.131764
[E036]  loss: 0.175431
[E037]  loss: 0.215776
[E038]  loss: 0.146799
[E039]  loss: 0.168955
[E040]  loss: 0.225336
[E041]  loss: 0.203506
[E042]  loss: 0.101058
[E043]  loss: 0.248873
[E044]  loss: 0.159038
[E045]  loss: 0.181890
[E046]  loss: 0.206626
[E047]  loss: 0.258476
[E048]  loss: 0.168118
[E049]  loss: 0.033122
[E050]  loss: 0.140294
[E051]  loss: 0.174546
[E052]  loss: 0.095061
[E053]  loss: 0.034240
[E054]  loss: 0.032410
[E055]  loss: 0.217185
[E056]  loss: 0.116683
[E057]  loss: 0.060625
[E058]  loss: 0.064702
[E059]  loss: 0.140544
[E060]  loss: 0.157059
[E061]  loss: 0.072244
[E062]  loss: 0.168902
[E063]  loss: 0.106819
[E064]  loss: 0.105091
[E065]  loss: 0.042700
[E066]  loss: 0.030980
[E067]  loss: 0.060733
[E068]  loss: 0.031292
[E069]  loss: 0.098037
Training finished!... save training results
Total time: 1479.7684423923492
Per epoch time: [29.836707830429077, 19.417291402816772, 19.07362127304077, 19.979660272598267, 19.307941913604736, 20.510783672332764, 20.042142391204834, 19.43291425704956, 19.479777574539185, 19.979657888412476, 20.08900761604309, 19.042383670806885, 19.964037656784058, 19.82344365119934, 19.776581525802612, 20.07338786125183, 19.698474645614624, 20.042142868041992, 20.41705346107483, 20.19281244277954, 19.479777097702026, 20.338950872421265, 19.073623180389404, 19.479776620864868, 19.29232096672058, 19.229835510253906, 19.776580810546875, 19.885932445526123, 19.1048641204834, 20.167115449905396, 19.511018991470337, 19.136106967926025, 19.02676010131836, 19.97965693473816, 19.073622226715088, 19.29232120513916, 19.775804042816162, 19.18297290802002, 20.026522636413574, 19.245456218719482, 19.63479447364807, 19.76095962524414, 19.214213371276855, 20.995042085647583, 20.635753631591797, 20.60451364517212, 20.01090121269226, 19.806310653686523, 21.182499647140503, 19.92032766342163, 20.09271740913391, 20.022328853607178, 19.896401166915894, 19.59480905532837, 19.542260885238647, 19.664942264556885, 19.87030863761902, 19.33918261528015, 20.151493787765503, 20.12024998664856, 19.276700019836426, 19.307944774627686, 19.57348942756653, 19.151728868484497, 19.386048316955566, 19.678416967391968, 19.417289972305298, 19.60474705696106, 19.63598942756653, 19.276699542999268]

[E000]  loss: 0.693289
[E001]  loss: 0.773751
[E002]  loss: 0.493654
[E003]  loss: 0.338411
[E004]  loss: 0.603068
[E005]  loss: 0.382144
[E006]  loss: 0.518566
[E007]  loss: 0.499916
[E008]  loss: 0.357541
[E009]  loss: 0.382676
[E010]  loss: 0.417245
[E011]  loss: 0.267659
[E012]  loss: 0.398113
[E013]  loss: 0.441030
[E014]  loss: 0.302610
[E015]  loss: 0.222716
[E016]  loss: 0.243957
[E017]  loss: 0.258161
[E018]  loss: 0.269804
[E019]  loss: 0.302891
[E020]  loss: 0.285186
[E021]  loss: 0.262926
[E022]  loss: 0.318924
[E023]  loss: 0.192974
[E024]  loss: 0.212323
[E025]  loss: 0.261433
[E026]  loss: 0.278031
[E027]  loss: 0.367780
[E028]  loss: 0.282869
[E029]  loss: 0.349747
[E030]  loss: 0.170443
[E031]  loss: 0.163203
[E032]  loss: 0.155699
[E033]  loss: 0.318005
[E034]  loss: 0.245991
[E035]  loss: 0.192305
[E036]  loss: 0.172290
[E037]  loss: 0.234689
[E038]  loss: 0.088946
[E039]  loss: 0.108622
[E040]  loss: 0.150150
[E041]  loss: 0.253278
[E042]  loss: 0.151451
[E043]  loss: 0.160444
[E044]  loss: 0.092442
[E045]  loss: 0.104457
[E046]  loss: 0.185701
[E047]  loss: 0.210267
[E048]  loss: 0.129125
[E049]  loss: 0.163171
[E050]  loss: 0.173082
[E051]  loss: 0.047357
[E052]  loss: 0.073658
[E053]  loss: 0.180402
[E054]  loss: 0.086977
[E055]  loss: 0.061028
[E056]  loss: 0.077246
[E057]  loss: 0.057228
[E058]  loss: 0.062660
[E059]  loss: 0.058492
[E060]  loss: 0.083392
[E061]  loss: 0.037961
[E062]  loss: 0.105265
[E063]  loss: 0.085512
[E064]  loss: 0.045120
[E065]  loss: 0.124520
[E066]  loss: 0.092489
[E067]  loss: 0.056513
[E068]  loss: 0.140787
[E069]  loss: 0.087529
Training finished!... save training results
Total time: 507.6259524822235
Per epoch time: [18.38628339767456, 6.8108930587768555, 6.6859235763549805, 6.998347997665405, 7.0907275676727295, 6.780187129974365, 7.42012357711792, 7.04521369934082, 6.717165470123291, 6.7952721118927, 6.623436450958252, 6.560953140258789, 6.654681921005249, 6.8108930587768555, 6.7640297412872314, 6.6234376430511475, 6.764028549194336, 6.670302391052246, 7.107700824737549, 6.877090930938721, 6.795269727706909, 6.889000654220581, 6.748408555984497, 7.1545634269714355, 6.685922145843506, 6.857757806777954, 6.701543807983398, 6.920244455337524, 6.670301675796509, 6.748408079147339, 6.6234354972839355, 6.639057397842407, 6.796606063842773, 6.87337851524353, 6.764029026031494, 6.7952721118927, 6.717164993286133, 6.670300722122192, 6.935863733291626, 6.842135429382324, 6.717164516448975, 6.576572895050049, 7.185805320739746, 6.748408317565918, 6.9827282428741455, 6.529714345932007, 6.498466968536377, 6.857757329940796, 6.654679298400879, 6.982727766036987, 6.764030456542969, 6.748409271240234, 6.654678821563721, 6.6234352588653564, 6.779650926589966, 6.951485633850098, 6.888999700546265, 6.842135906219482, 7.13893985748291, 6.76403021812439, 6.685924291610718, 7.045212030410767, 6.826514482498169, 6.795272350311279, 6.732785224914551, 6.935863971710205, 6.739978790283203, 6.779650449752808, 6.7640297412872314, 6.6703009605407715]

[*] Training finished