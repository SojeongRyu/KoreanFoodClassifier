comment: data_clean_70epoch
model and loss plot -> ./model/model_data_clean_70epoch
Namespace(batch_size=200, beta1=0.5, beta2=0.999, comment='data_clean_70epoch', dataroot_dir='./data_clean', epoch=70, fold_num=-1, gpu_mode=True, lr=0.0002, num_workers=4, result_dir='./results/', sample_num=64, save_dir='./model/', type='train')
[*] Training started
[E000]  loss: 2.330721
[E001]  loss: 2.272716
[E002]  loss: 3.268495
[E003]  loss: 2.022071
[E004]  loss: 1.562814
[E005]  loss: 1.467869
[E006]  loss: 1.386770
[E007]  loss: 1.197875
[E008]  loss: 1.415214
[E009]  loss: 1.286715
[E010]  loss: 1.034869
[E011]  loss: 1.087931
[E012]  loss: 0.680963
[E013]  loss: 0.841565
[E014]  loss: 0.908970
[E015]  loss: 0.867573
[E016]  loss: 0.783324
[E017]  loss: 0.776881
[E018]  loss: 0.736994
[E019]  loss: 0.828747
[E020]  loss: 0.667454
[E021]  loss: 0.701187
[E022]  loss: 0.629411
[E023]  loss: 0.741698
[E024]  loss: 0.702646
[E025]  loss: 0.608886
[E026]  loss: 0.549035
[E027]  loss: 0.719346
[E028]  loss: 0.473720
[E029]  loss: 0.450265
[E030]  loss: 0.404483
[E031]  loss: 0.414691
[E032]  loss: 0.413480
[E033]  loss: 0.442491
[E034]  loss: 0.277632
[E035]  loss: 0.366254
[E036]  loss: 0.314474
[E037]  loss: 0.296914
[E038]  loss: 0.239956
[E039]  loss: 0.283853
[E040]  loss: 0.147813
[E041]  loss: 0.254267
[E042]  loss: 0.291194
[E043]  loss: 0.252591
[E044]  loss: 0.267360
[E045]  loss: 0.292804
[E046]  loss: 0.179811
[E047]  loss: 1.371666
[E048]  loss: 0.170349
[E049]  loss: 0.217711
[E050]  loss: 0.133115
[E051]  loss: 0.161688
[E052]  loss: 0.154321
[E053]  loss: 0.185984
[E054]  loss: 0.110843
[E055]  loss: 0.191734
[E056]  loss: 0.110862
[E057]  loss: 0.123461
[E058]  loss: 0.071963
[E059]  loss: 0.137260
[E060]  loss: 0.116145
[E061]  loss: 0.124427
[E062]  loss: 0.168154
[E063]  loss: 0.039119
[E064]  loss: 0.230084
[E065]  loss: 0.086695
[E066]  loss: 0.054476
[E067]  loss: 0.174618
[E068]  loss: 0.252078
[E069]  loss: 0.077034
Training finished!... save training results
Total time: 2080.656576395035
Per epoch time: [20.49372386932373, 19.9168758392334, 19.995739698410034, 19.523547649383545, 19.800811767578125, 19.36978793144226, 19.74377131462097, 20.63409185409546, 19.241819620132446, 19.83652353286743, 20.45602798461914, 19.496763467788696, 19.955068588256836, 19.628699779510498, 20.0651798248291, 19.723435878753662, 19.34746789932251, 19.600427627563477, 19.576619863510132, 19.489819526672363, 19.420379877090454, 19.86330795288086, 416.94197726249695, 20.790331840515137, 61.89285135269165, 20.151979446411133, 20.83546805381775, 31.919081449508667, 20.399483680725098, 20.961947441101074, 19.6907000541687, 20.25068426132202, 166.4878213405609, 19.08111572265625, 20.988731622695923, 20.293835401535034, 21.18861961364746, 19.485851764678955, 20.53092336654663, 20.107340097427368, 19.53148365020752, 19.46799612045288, 19.576619625091553, 20.74172353744507, 19.828588008880615, 19.15650773048401, 19.73335576057434, 19.46700382232666, 19.818667888641357, 19.697643756866455, 20.57457160949707, 19.58703589439392, 20.074604034423828, 19.11335587501526, 19.763115882873535, 20.445611715316772, 19.28348422050476, 19.599931716918945, 20.105851888656616, 19.569676399230957, 19.264139652252197, 20.380635499954224, 19.134187936782837, 19.82412338256836, 19.181308269500732, 22.287755727767944, 19.416412115097046, 20.293835878372192, 20.211003303527832, 19.509164333343506]

[*] Training finished