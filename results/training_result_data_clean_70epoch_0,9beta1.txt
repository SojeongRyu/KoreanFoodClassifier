comment: data_clean_70epoch_0.9beta1
model and loss plot -> ./model/model_data_clean_70epoch_0.9beta1
Namespace(batch_size=100, beta1=0.9, beta2=0.999, comment='data_clean_70epoch_0.9beta1', dataroot_dir='./data_clean', epoch=70, fold_num=-1, gpu_mode=True, lr=0.0002, num_workers=4, result_dir='./results/', sample_num=64, save_dir='./model/', type='train')
[*] Training started
[E000]  loss: 2.304780
[E001]  loss: 1.569467
[E002]  loss: 1.287791
[E003]  loss: 0.981031
[E004]  loss: 1.032504
[E005]  loss: 0.821507
[E006]  loss: 0.723942
[E007]  loss: 0.708066
[E008]  loss: 0.616099
[E009]  loss: 0.768548
[E010]  loss: 0.806562
[E011]  loss: 0.619607
[E012]  loss: 0.588573
[E013]  loss: 0.519328
[E014]  loss: 0.425249
[E015]  loss: 0.383047
[E016]  loss: 0.406300
[E017]  loss: 0.417395
[E018]  loss: 0.387477
[E019]  loss: 0.239861
[E020]  loss: 0.338968
[E021]  loss: 0.277400
[E022]  loss: 0.392704
[E023]  loss: 0.208499
[E024]  loss: 0.251127
[E025]  loss: 0.359662
[E026]  loss: 0.257291
[E027]  loss: 0.308285
[E028]  loss: 0.296612
[E029]  loss: 0.178222
[E030]  loss: 0.275132
[E031]  loss: 0.106857
[E032]  loss: 0.305963
[E033]  loss: 0.122005
[E034]  loss: 0.077924
[E035]  loss: 0.119319
[E036]  loss: 0.112950
[E037]  loss: 0.198574
[E038]  loss: 0.136059
[E039]  loss: 0.240653
[E040]  loss: 0.277037
[E041]  loss: 0.081061
[E042]  loss: 0.198827
[E043]  loss: 0.115366
[E044]  loss: 0.164270
[E045]  loss: 0.206159
[E046]  loss: 0.267727
[E047]  loss: 0.273932
[E048]  loss: 0.058815
[E049]  loss: 0.113697
[E050]  loss: 0.128857
[E051]  loss: 0.049715
[E052]  loss: 0.145582
[E053]  loss: 0.163822
[E054]  loss: 0.085178
[E055]  loss: 0.086704
[E056]  loss: 0.126237
[E057]  loss: 0.072657
[E058]  loss: 0.136890
[E059]  loss: 0.277199
[E060]  loss: 0.081147
[E061]  loss: 0.196671
[E062]  loss: 0.056067
[E063]  loss: 0.076600
[E064]  loss: 0.080736
[E065]  loss: 0.105652
[E066]  loss: 0.103777
[E067]  loss: 0.038199
[E068]  loss: 0.165158
[E069]  loss: 0.071915
Training finished!... save training results
Total time: 1499.5931520462036
Per epoch time: [58.403759241104126, 20.39842987060547, 19.276943922042847, 19.305355072021484, 19.641443014144897, 19.67735981941223, 19.50282645225525, 19.833940982818604, 19.604554653167725, 19.832943439483643, 19.683343172073364, 19.79105520248413, 19.4170560836792, 20.195971965789795, 19.1627357006073, 19.50282645225525, 19.263466596603394, 19.41705584526062, 19.83094835281372, 19.731215476989746, 19.75714635848999, 19.39910387992859, 20.036399126052856, 20.09524154663086, 19.24551486968994, 19.55368995666504, 19.71525812149048, 19.644447088241577, 20.65873432159424, 19.22157859802246, 20.416382312774658, 20.00448441505432, 19.755151510238647, 19.702292680740356, 19.055023908615112, 19.96459150314331, 20.033406734466553, 19.396113634109497, 19.041061639785767, 19.690325021743774, 20.277753114700317, 19.22656512260437, 19.827956438064575, 19.823967456817627, 19.449967861175537, 19.546708583831787, 19.997518301010132, 20.442312955856323, 19.678357124328613, 19.884804487228394, 19.855392456054688, 19.337269067764282, 20.39444065093994, 19.15874671936035, 19.665391445159912, 20.460264921188354, 19.863860607147217, 20.391448974609375, 19.912729263305664, 19.771109104156494, 19.700298070907593, 19.779540300369263, 19.589126110076904, 19.073622703552246, 19.386048555374146, 18.52687644958496, 18.620604038238525, 18.91740894317627, 18.90178680419922, 18.65184760093689]

[*] Training finished