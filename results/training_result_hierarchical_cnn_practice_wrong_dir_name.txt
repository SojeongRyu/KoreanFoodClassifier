(SSS) C:\Users\405B\KFC>python main.py --comment hierarchical_cnn_practice --dataroot_dir ./data_hierarchy_practice/ --epoch 30
comment: hierarchical_cnn_practice
model and loss plot -> ./model/model_hierarchical_cnn_practice
Namespace(batch_size=100, beta1=0.5, beta2=0.999, comment='hierarchical_cnn_practice', dataroot_dir='./data_hierarchy_practice/', epoch=30, gpu_mode=True, lr=0.0002, num_workers=4, result_dir='./results/', sample_num=64, save_dir='./model/', type='train')
[*] Training started


<model 0>
[E000]  loss: 1.793430
[E001]  loss: 1.209917
[E002]  loss: 1.296491
[E003]  loss: 1.146793
[E004]  loss: 1.114228
[E005]  loss: 1.011584
[E006]  loss: 0.933549
[E007]  loss: 1.062701
[E008]  loss: 0.898916
[E009]  loss: 1.143055
[E010]  loss: 0.908043
[E011]  loss: 0.617125
[E012]  loss: 0.638426
[E013]  loss: 0.504943
[E014]  loss: 0.566422
[E015]  loss: 0.499613
[E016]  loss: 0.526931
[E017]  loss: 0.619763
[E018]  loss: 0.683654
[E019]  loss: 0.449515
[E020]  loss: 0.397312
[E021]  loss: 0.302315
[E022]  loss: 0.390717
[E023]  loss: 0.337414
[E024]  loss: 0.299302
[E025]  loss: 0.216217
[E026]  loss: 0.194575
[E027]  loss: 0.366474
[E028]  loss: 0.270348
[E029]  loss: 0.202672
Training finished!... save training results
Total time: 881.499764919281
Per epoch time: [48.112343311309814, 27.931308031082153, 28.158700704574585, 29.77240490913391, 27.640087366104126, 27.37241768836975, 26.937581062316895, 27.500224828720093, 29.1094913482666, 27.609464406967163, 28.125248670578003, 26.875736713409424, 27.03145718574524, 26.610037803649902, 28.703046798706055, 26.23474144935608, 27.017552614212036, 26.71894359588623, 28.90860939025879, 26.85789465904236, 27.844213247299194, 27.609615564346313, 27.640806436538696, 26.843095779418945, 26.905863761901855, 27.093339204788208, 26.733718872070312, 26.735210418701172, 26.65613555908203, 27.28161120414734]


<model 1>
[E000]  loss: 1.105406
[E001]  loss: 1.010292
[E002]  loss: 0.905488
[E003]  loss: 1.079951
[E004]  loss: 0.986136
[E005]  loss: 0.924095
[E006]  loss: 0.789433
[E007]  loss: 1.113129
[E008]  loss: 0.975465
[E009]  loss: 0.765413
[E010]  loss: 1.019635
[E011]  loss: 0.698467
[E012]  loss: 0.752291
[E013]  loss: 0.684292
[E014]  loss: 0.740964
[E015]  loss: 0.608180
[E016]  loss: 0.788232
[E017]  loss: 0.546466
[E018]  loss: 0.630897
[E019]  loss: 0.728910
[E020]  loss: 0.588742
[E021]  loss: 0.727916
[E022]  loss: 0.652444
[E023]  loss: 0.543644
[E024]  loss: 0.650925
[E025]  loss: 0.775841
[E026]  loss: 0.679586
[E027]  loss: 0.525751
[E028]  loss: 0.637462
[E029]  loss: 0.523353
Training finished!... save training results
Total time: 571.5731160640717
Per epoch time: [10.950716733932495, 10.373260498046875, 9.317085027694702, 9.870604753494263, 9.37891960144043, 9.51754903793335, 9.656177997589111, 9.661164045333862, 9.761895418167114, 9.367948770523071, 9.415821313858032, 9.491618633270264, 9.885564804077148, 9.96235966682434, 9.663159847259521, 9.738957166671753, 9.926453828811646, 9.474663734436035, 9.58038067817688, 9.86262583732605, 9.447271347045898, 9.73696231842041, 9.582375526428223, 9.351991653442383, 254.74281644821167, 9.496605157852173, 9.39687204360962, 9.455714464187622, 9.594343900680542, 9.598332643508911]


<model 2>
[E000]  loss: 1.123068
[E001]  loss: 1.057799
[E002]  loss: 0.984585
[E003]  loss: 1.083861
[E004]  loss: 0.850607
[E005]  loss: 0.846124
[E006]  loss: 0.902583
[E007]  loss: 0.680912
[E008]  loss: 0.774984
[E009]  loss: 0.755279
[E010]  loss: 0.927455
[E011]  loss: 0.747406
[E012]  loss: 0.842574
[E013]  loss: 0.680481
[E014]  loss: 0.704045
[E015]  loss: 0.807382
[E016]  loss: 0.499529
[E017]  loss: 0.542527
[E018]  loss: 0.597287
[E019]  loss: 0.538834
[E020]  loss: 0.526891
[E021]  loss: 0.506794
[E022]  loss: 0.509922
[E023]  loss: 0.368363
[E024]  loss: 0.300357
[E025]  loss: 0.347098
[E026]  loss: 0.484861
[E027]  loss: 0.445359
[E028]  loss: 0.480320
[E029]  loss: 0.306536
Training finished!... save training results
Total time: 343.0800938606262
Per epoch time: [16.01418924331665, 28.924652576446533, 9.386898279190063, 10.09500527381897, 9.075729846954346, 9.140556812286377, 10.039153575897217, 9.831708908081055, 9.333550453186035, 9.734967947006226, 9.65119218826294, 8.991954326629639, 9.013896226882935, 9.12759256362915, 9.54048776626587, 9.399863958358765, 9.205383777618408, 9.060770273208618, 8.903192281723022, 9.571404933929443, 9.023868560791016, 9.211367130279541, 9.078722715377808, 9.23430609703064, 9.305116415023804, 9.262231349945068, 9.04481291770935, 9.198402404785156, 8.908179759979248, 9.196407318115234]