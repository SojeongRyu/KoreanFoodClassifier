comment: data_clean_subnet0+4_test2
model and loss plot -> ./model/model_data_clean_subnet0+4_test2
Namespace(batch_size=100, beta1=0.9, beta2=0.999, comment='data_clean_subnet0+4_test2', dataroot_dir='./data_clean', epoch=70, fold_num=-1, gpu_mode=True, lr=0.0002, num_workers=4, result_dir='./results/', sample_num=64, save_dir='./model/', sub_net_name='0+4', type='train')
[*] Training started
[E000]  loss: 2.404472
[E001]  loss: 1.642763
[E002]  loss: 1.200710
[E003]  loss: 1.118452
[E004]  loss: 1.025090
[E005]  loss: 0.825767
[E006]  loss: 0.777928
[E007]  loss: 0.859925
[E008]  loss: 0.568414
[E009]  loss: 0.814466
[E010]  loss: 0.646072
[E011]  loss: 0.791475
[E012]  loss: 0.428986
[E013]  loss: 0.509636
[E014]  loss: 0.508598
[E015]  loss: 0.518409
[E016]  loss: 0.355794
[E017]  loss: 0.478188
[E018]  loss: 0.387569
[E019]  loss: 0.298803
[E020]  loss: 0.299946
[E021]  loss: 0.237059
[E022]  loss: 0.273046
[E023]  loss: 0.251288
[E024]  loss: 0.238663
[E025]  loss: 0.432169
[E026]  loss: 0.297705
[E027]  loss: 0.277084
[E028]  loss: 0.333412
[E029]  loss: 0.198878
[E030]  loss: 0.199178
[E031]  loss: 0.309487
[E032]  loss: 0.382124
[E033]  loss: 0.167509
[E034]  loss: 0.169862
[E035]  loss: 0.169883
[E036]  loss: 0.123137
[E037]  loss: 0.110997
[E038]  loss: 0.098114
[E039]  loss: 0.136571
[E040]  loss: 0.162987
[E041]  loss: 0.209411
[E042]  loss: 0.067412
[E043]  loss: 0.057778
[E044]  loss: 0.157335
[E045]  loss: 0.059393
[E046]  loss: 0.110087
[E047]  loss: 0.101312
[E048]  loss: 0.126094
[E049]  loss: 0.139610
[E050]  loss: 0.120577
[E051]  loss: 0.127047
[E052]  loss: 0.077932
[E053]  loss: 0.104088
[E054]  loss: 0.101051
[E055]  loss: 0.102682
[E056]  loss: 0.110340
[E057]  loss: 0.068320
[E058]  loss: 0.071148
[E059]  loss: 0.061350
[E060]  loss: 0.114366
[E061]  loss: 0.163676
[E062]  loss: 0.072937
[E063]  loss: 0.140306
[E064]  loss: 0.096564
[E065]  loss: 0.080427
[E066]  loss: 0.039611
[E067]  loss: 0.044561
[E068]  loss: 0.018418
[E069]  loss: 0.099745
Training finished!... save training results
Total time: 1688.8042285442352
Per epoch time: [68.54064106941223, 20.396435260772705, 21.260125398635864, 20.74649953842163, 20.039897441864014, 19.77410078048706, 19.753156900405884,
 20.474227905273438, 19.45794677734375, 20.708601236343384, 19.705284595489502, 20.064324140548706, 20.06631898880005, 20.019444227218628, 19.77110862731
9336, 20.75647258758545, 19.82695960998535, 20.30867099761963, 20.21990704536438, 19.910735368728638, 19.651428699493408, 20.105215072631836, 19.91822481
1553955, 19.591100692749023, 19.9391667842865, 19.634474515914917, 20.485706329345703, 20.185001134872437, 20.064324140548706, 19.704286813735962, 20.234
867811203003, 19.766119480133057, 20.3964364528656, 19.52926206588745, 20.188990831375122, 20.216426372528076, 19.886799097061157, 21.796690225601196, 20
.74450421333313, 20.117183208465576, 21.013784170150757, 21.271095991134644, 24.574259757995605, 24.372798681259155, 24.725855588912964, 25.6104869842529
3, 27.903353452682495, 25.049987077713013, 24.861488819122314, 24.53786563873291, 24.097535848617554, 26.05729341506958, 24.941277742385864, 23.279722690
582275, 20.548044443130493, 37.7549991607666, 21.73385787010193, 21.509458541870117, 21.71590542793274, 21.410722732543945, 21.60320782661438, 20.5021531
58187866, 21.852539539337158, 21.254140853881836, 21.416706323623657, 21.353875160217285, 22.448944568634033, 21.541373014450073, 23.592884063720703, 22.
37813425064087]

[E000]  loss: 0.708895
[E001]  loss: 0.753807
[E002]  loss: 0.487369
[E003]  loss: 0.439867
[E004]  loss: 0.486987
[E005]  loss: 0.494764
[E006]  loss: 0.465401
[E007]  loss: 0.373232
[E008]  loss: 0.363920
[E009]  loss: 0.495581
[E010]  loss: 0.309290
[E011]  loss: 0.341920
[E012]  loss: 0.261877
[E013]  loss: 0.293424
[E014]  loss: 0.299389
[E015]  loss: 0.300988
[E016]  loss: 0.208055
[E017]  loss: 0.218062
[E018]  loss: 0.214362
[E019]  loss: 0.236256
[E020]  loss: 0.217167
[E021]  loss: 0.242310
[E022]  loss: 0.129087
[E023]  loss: 0.258323
[E024]  loss: 0.344012
[E025]  loss: 0.240170
[E026]  loss: 0.220008
[E027]  loss: 0.337597
[E028]  loss: 0.099368
[E029]  loss: 0.180918
[E030]  loss: 0.277338
[E031]  loss: 0.157648
[E032]  loss: 0.208700
[E033]  loss: 0.332173
[E034]  loss: 0.148608
[E035]  loss: 0.154485
[E036]  loss: 0.207572
[E037]  loss: 0.082937
[E038]  loss: 0.105020
[E039]  loss: 0.125238
[E040]  loss: 0.203025
[E041]  loss: 0.061269
[E042]  loss: 0.150163
[E043]  loss: 0.279811
[E044]  loss: 0.136079
[E045]  loss: 0.109191
[E046]  loss: 0.089875
[E047]  loss: 0.063128
[E048]  loss: 0.212905
[E049]  loss: 0.070894
[E050]  loss: 0.090953
[E051]  loss: 0.061756
[E052]  loss: 0.148346
[E053]  loss: 0.041426
[E054]  loss: 0.038375
[E055]  loss: 0.071279
[E056]  loss: 0.040214
[E057]  loss: 0.058812
[E058]  loss: 0.062691
[E059]  loss: 0.027612
[E060]  loss: 0.136920
[E061]  loss: 0.105378
[E062]  loss: 0.062989
[E063]  loss: 0.028255
[E064]  loss: 0.021389
[E065]  loss: 0.014649
[E066]  loss: 0.007845
[E067]  loss: 0.059002
[E068]  loss: 0.081738
[E069]  loss: 0.030337
Training finished!... save training results
Total time: 771.4048657417297
Per epoch time: [20.620835781097412, 8.691747426986694, 7.612637519836426, 6.989302158355713, 7.364299774169922, 7.674469470977783, 7.304458856582642, 7.
723338842391968, 8.968010425567627, 8.655843257904053, 7.754256010055542, 9.660158395767212, 8.857306718826294, 10.533820152282715, 8.721667528152466, 9.
274190187454224, 8.716682195663452, 8.522201538085938, 8.899192571640015, 8.236965656280518, 9.416808843612671, 8.30478310585022, 7.309445858001709, 7.41
7157888412476, 10.533820629119873, 10.584684133529663, 8.90418004989624, 8.840352773666382, 9.186424970626831, 9.15052080154419, 8.719673156738281, 7.710
3729248046875, 7.191761493682861, 7.176800966262817, 7.681450366973877, 7.7502665519714355, 7.472010612487793, 7.823071718215942, 7.51689076423645, 8.020
543336868286, 7.5178892612457275, 7.529856204986572, 7.870943784713745, 7.654524087905884, 7.553792476654053, 7.615626335144043, 7.92978572845459, 8.0524
5852470398, 7.728325366973877, 7.685439586639404, 7.221680402755737, 7.87792444229126, 7.517888307571411, 7.672474384307861, 8.32572627067566, 7.54980397
2244263, 7.545814037322998, 7.7243359088897705, 7.836036920547485, 8.12477731704712, 7.711371898651123, 7.733311891555786, 7.279525995254517, 7.511904001
235962, 7.155855894088745, 7.3513336181640625, 7.838031053543091, 7.3842453956604, 7.764230728149414, 8.313759565353394]

[*] Training finished
