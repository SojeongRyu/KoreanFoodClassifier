comment: data_clean_samenum2
model and loss plot -> ./model/model_data_clean_samenum2
Namespace(batch_size=100, beta1=0.5, beta2=0.999, comment='data_clean_samenum2', dataroot_dir='./data_clean_samenum', epoch=30, gpu_mode=True, lr=0.0002, num_workers=4, result_dir='./results/', sample_num=64, save_dir='./model/', type='train')
[*] Training started
[E000]	loss: 2.317440
[E001]	loss: 1.911784
[E002]	loss: 1.639849
[E003]	loss: 1.555472
[E004]	loss: 1.262649
[E005]	loss: 1.265897
[E006]	loss: 1.183674
[E007]	loss: 1.273957
[E008]	loss: 0.834483
[E009]	loss: 1.003138
[E010]	loss: 0.868104
[E011]	loss: 0.931010
[E012]	loss: 0.675053
[E013]	loss: 0.839073
[E014]	loss: 0.554967
[E015]	loss: 0.564836
[E016]	loss: 0.677286
[E017]	loss: 0.434339
[E018]	loss: 0.401048
[E019]	loss: 0.383541
[E020]	loss: 0.445758
[E021]	loss: 0.474828
[E022]	loss: 0.362200
[E023]	loss: 0.495023
[E024]	loss: 0.231312
[E025]	loss: 0.454860
[E026]	loss: 0.562177
[E027]	loss: 0.246177
[E028]	loss: 0.287008
[E029]	loss: 0.157967
Training finished!... save training results
Total time: 546.6633110046387
Per epoch time: [18.521132469177246, 16.667580604553223, 17.028172492980957, 16.4875328540802, 16.617980480194092, 15.891340732574463, 17.218141078948975, 16.506876230239868, 16.396268844604492, 16.434460401535034, 16.561932802200317, 17.785564184188843, 16.26383638381958, 17.320812225341797, 16.483564138412476, 16.953277349472046, 16.82332468032837, 16.81241226196289, 17.804412603378296, 16.689404487609863, 16.610044717788696, 16.733549118041992, 16.64625310897827, 16.31839632987976, 15.896796464920044, 16.788604259490967, 17.092652559280396, 17.353548288345337, 15.85265302658081, 16.960715532302856]
[*] Training finished
